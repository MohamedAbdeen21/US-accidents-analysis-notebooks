{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2372dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import *\n",
    "from config import SOURCE_TOPIC, SERVER_PORT, DATADIR\n",
    "\n",
    "spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .master(\"local[6]\") \\\n",
    "          .appName(\"Kafka to mongodb\") \\\n",
    "          .getOrCreate()\n",
    "\n",
    "df = spark\\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", SERVER_PORT) \\\n",
    "      .option(\"subscribe\", SOURCE_TOPIC) \\\n",
    "      .option(\"startingOffsets\", \"latest\") \\\n",
    "      .option('failOnDataLoss','false') \\\n",
    "      .load() \\\n",
    "      .select('value','timestamp')\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db706e87",
   "metadata": {},
   "source": [
    "For this part, defining schema is a MUST. In order to explode a column of json to multiple columns, pyspark needs to know the schema. If a datatype doesn't match, pyspark will resort to returning nulls.\n",
    "\n",
    "I don't need to say that I didn't type that by hand. We have ✨ vim ✨ magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1af0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = StructType([\n",
    "    StructField('Event',StringType() , False),\n",
    "    StructField('Stream_Time', DoubleType() , False),\n",
    "    StructField('ID', IntegerType() , False),\n",
    "    StructField('Severity',ByteType() , False),\n",
    "    StructField('Start_Time',TimestampType() , False),\n",
    "    StructField('End_Time',TimestampType() , False),\n",
    "    StructField('Start_Lat',DoubleType() , False),\n",
    "    StructField('Start_Lng',DoubleType() , False),\n",
    "    StructField('End_Lat',DoubleType() , False),\n",
    "    StructField('End_Lng',DoubleType() , False),\n",
    "    StructField('Distance_mi',DoubleType() , False),\n",
    "    StructField('Description',StringType() , False),\n",
    "    StructField('Number',StringType() , False),\n",
    "    StructField('Street',StringType() , False),\n",
    "    StructField('Side',StringType() , False),\n",
    "    StructField('City',StringType() , False),\n",
    "    StructField('County',StringType() , False),\n",
    "    StructField('State',StringType() , False),\n",
    "    StructField('Zipcode',StringType() , False),\n",
    "    StructField('Country',StringType() , False),\n",
    "    StructField('Timezone',StringType() , False),\n",
    "    StructField('Airport_Code',StringType() , False),\n",
    "    StructField('Weather_Timestamp',TimestampType() , False),\n",
    "    StructField('Temperature_F',DoubleType() , False),\n",
    "    StructField('Wind_Chill_F',DoubleType() , False),\n",
    "    StructField('Humidity_Percent',DoubleType() , False),\n",
    "    StructField('Pressure_in',DoubleType() , False),\n",
    "    StructField('Visibility_mi',DoubleType() , False),\n",
    "    StructField('Wind_Direction',StringType() , False),\n",
    "    StructField('Wind_Speed_mph',DoubleType() , False),\n",
    "    StructField('Precipitation_in',DoubleType() , False),\n",
    "    StructField('Weather_Condition',StringType() , False),\n",
    "    StructField('Amenity',BooleanType() , False),\n",
    "    StructField('Bump',BooleanType() , False),\n",
    "    StructField('Crossing',BooleanType() , False),\n",
    "    StructField('Give_Way',BooleanType() , False),\n",
    "    StructField('Junction',BooleanType() , False),\n",
    "    StructField('No_Exit',BooleanType() , False),\n",
    "    StructField('Railway',BooleanType() , False),\n",
    "    StructField('Roundabout',BooleanType() , False),\n",
    "    StructField('Station',BooleanType() , False),\n",
    "    StructField('Stop',BooleanType() , False),\n",
    "    StructField('Traffic_Calming',BooleanType() , False),\n",
    "    StructField('Traffic_Signal',BooleanType() , False),\n",
    "    StructField('Turning_Loop',BooleanType() , False),\n",
    "    StructField('Sunrise_Sunset',StringType() , False),\n",
    "    StructField('Civil_Twilight',StringType() , False),\n",
    "    StructField('Nautical_Twilight',StringType() , False),\n",
    "    StructField('Astronomical_Twilight',StringType(), False)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a6642f",
   "metadata": {},
   "source": [
    "remember that the data is dumped to string, so first cast as string and then read with `from_json`, then select the columns you need. For this one we need all the columns.\n",
    "\n",
    "For the mongo connection we define the host and port, the database, the collection (equivalent to table in relational databases). We don't bother creating the collection, mongo creates it when it sees us using it. checkpointLocation is a MUST.\n",
    "\n",
    "As always, this is a streaming script, therefore it needs to be run from terminal and as `.py`.\n",
    "\n",
    "The terminal command uses an extra package:\n",
    "`spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector:10.0.2 kafkamongo.py`\n",
    "\n",
    "Again, you'll have to google the compatible package version with your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df \\\n",
    "    .withColumn('json', df.value.cast(StringType())) \\\n",
    "    .withColumn('jsonData', from_json(col('json'), schema)) \\\n",
    "    .drop('json','value')\n",
    "\n",
    "new_df = df \\\n",
    "        .select('jsonData.*')\n",
    "\n",
    "query = new_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode('append') \\\n",
    "    .format('mongodb') \\\n",
    "    .option('spark.mongodb.connection.uri','mongodb://localhost:27017') \\\n",
    "    .option('spark.mongodb.database','mongodb') \\\n",
    "    .option('spark.mongodb.collection','accidents') \\\n",
    "    .option('checkpointLocation', DATADIR + 'temp/') \\\n",
    "    .start()\\\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe5f59",
   "metadata": {},
   "source": [
    "While this works, mongo write performance prefers fault-tolerance over speed, as mentioned [here](https://www.mongodb.com/docs/v4.2/core/write-performance/). Also, this is a free version of mongo, so I don't expect the optimal performance. \n",
    "\n",
    "Was considering using cassandra, maybe I'll revisit the project and try cassandra performance. But for now, we go back to postgreSQL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
