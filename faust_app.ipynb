{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39d03a4",
   "metadata": {},
   "source": [
    "Now that the data is written to a kafka topic, we need to do some processing. We can write a spark job to do this task, but since the source and sink are both kafka topics it's better to use kafka streams.\n",
    "\n",
    "This is where a problem rises. Kafka streams is only supported in Java, and I'm not as fluent in Java as I wish to be. Therefore, we will use faust which, per faust documentation, \"is a stream processing library, porting the ideas from Kafka Streams to Python.\"\n",
    "\n",
    "Faust requires to run from terminal, therefore this notebook will not contain any output. The code in this notebook will be put into a .py file and be executed from terminal.\n",
    "\n",
    "One thing that we \"want\" is the schema of our topic and I say \"want\" because it is not required. For that we can use some vim magic (actually it's regex magic, just done using vim). I'll include a quick guide of how we get the schema easily. \n",
    "\n",
    "First we need to copy the schema by calling `df.printSchema()` on the pyspark df from `prepare_stream` or `stream` scripts. Then we copy that and paste to vim using `Ctrl+Shift+v`.\n",
    "\n",
    "Then we clear the `|--` before each column by running `:%s/ |-- /`\n",
    "\n",
    "Next we remove the `(nullable = true)` by running `:%s/(n.*)/`\n",
    "\n",
    "Change 'string' to 'str' with `:%s/string/str`\n",
    "\n",
    "And change 'integer' to 'int' with `:%s/integer/int`\n",
    "\n",
    "Clean the units in columns' names with `:%s/(/_` and `:%s/)/` and remove the '\\_%' from humidity column name.\n",
    "\n",
    "Then copy all the result using your mouse (just easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68028e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that faust is kind of dead. There is a community-maintained fork called faust-streaming\n",
    "!pip install faust-streaming\n",
    "import faust\n",
    "from config import SOURCE_TOPIC, SERVER_PORT, START_EVENTS_TOPIC, END_EVENTS_TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ce85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class row(faust.Record, validation = True):\n",
    "    Start: str \n",
    "    Stream_Time: float \n",
    "    ID: int \n",
    "    Severity: str \n",
    "    Start_Time: str \n",
    "    End_Time: str \n",
    "    Start_Lat: str \n",
    "    Start_Lng: str \n",
    "    End_Lat: str \n",
    "    End_Lng: str \n",
    "    Distance_mi: str \n",
    "    Description: str \n",
    "    Number: str \n",
    "    Street: str \n",
    "    Side: str \n",
    "    City: str \n",
    "    County: str \n",
    "    State: str \n",
    "    Zipcode: str \n",
    "    Country: str \n",
    "    Timezone: str \n",
    "    Airport_Code: str \n",
    "    Weather_Timestamp: str \n",
    "    Temperature_F: str \n",
    "    Wind_Chill_F: str \n",
    "    Humidity: str \n",
    "    Pressure_in: str \n",
    "    Visibility_mi: str \n",
    "    Wind_Direction: str \n",
    "    Wind_Speed_mph: str \n",
    "    Precipitation_in: str \n",
    "    Weather_Condition: str \n",
    "    Amenity: str \n",
    "    Bump: str \n",
    "    Crossing: str \n",
    "    Give_Way: str \n",
    "    Junction: str \n",
    "    No_Exit: str \n",
    "    Railway: str \n",
    "    Roundabout: str \n",
    "    Station: str \n",
    "    Stop: str \n",
    "    Traffic_Calming: str \n",
    "    Traffic_Signal: str \n",
    "    Turning_Loop: str \n",
    "    Sunrise_Sunset: str \n",
    "    Civil_Twilight: str \n",
    "    Nautical_Twilight: str \n",
    "    Astronomical_Twilight: str "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915b937",
   "metadata": {},
   "source": [
    "In our final versions, we will move this to a separate module.\n",
    "\n",
    "The class above is the schema of each accident, but remember that the producer to `SOURCE_TOPIC` sends a list of accidents. Therefore, we need to pass `value_type = List[Rows]` to the source topic. Unfortuantely, I don't know how to do this, so we will skip this part for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa94140",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = faust.App('Divider',broker = f\"kafka://{SERVER_PORT}\")\n",
    "source_topic = app.topic(TOPIC,value_type=list[row])\n",
    "start_topic = app.topic(START_EVENTS_TOPIC, value_type = row)\n",
    "end_topic = app.topic(END_EVENTS_TOPIC, value_type = row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f43158",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.agent(source_topic)\n",
    "async def end_reading(records):\n",
    "   async for record in records:\n",
    "        async for event in record:\n",
    "            if event['Start'] == 'Start':\n",
    "                await start_event.send(value = event)\n",
    "            else:\n",
    "                await end_event.send(value = event)\n",
    "app.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca919225",
   "metadata": {},
   "source": [
    "You will have to take my word for it when I tell you that this works. But I promise it does.\n",
    "\n",
    "This is the ingestion layer done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
