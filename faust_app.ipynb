{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39d03a4",
   "metadata": {},
   "source": [
    "Now that the data is written to a kafka topic, we need to do some processing. We can write a spark job to do this task, but since the source and sink are both kafka topics it's better to use kafka streams.\n",
    "\n",
    "This is where a problem rises. Kafka streams is only supported in Java, and I'm not as fluent in Java as I wish to be. Therefore, we will use faust which, per faust documentation, \"is a stream processing library, porting the ideas from Kafka Streams to Python.\"\n",
    "\n",
    "Faust requires to run from terminal, therefore this notebook will not contain any output. The code in this notebook will be put into a .py file and be executed from terminal.\n",
    "\n",
    "One thing that we \"want\" is the schema of our topic and I say \"want\" because it is not required. For that we can use some vim magic (actually it's regex magic, just done using vim). I'll include a quick guide of how we get the schema easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb55e4",
   "metadata": {},
   "source": [
    "First we need to copy the schema by calling `stream_df.printSchema()` on the pyspark df from `prepare_stream` or `stream` scripts. Then we copy that and paste to vim using `Ctrl+Shift+v`.\n",
    "\n",
    "Then we clear the `|--` before each column by running `:%s/ |-- /`\n",
    "\n",
    "Next we remove the `(nullable = true)` by running `:%s/(n.*)/`\n",
    "\n",
    "Change 'string' to 'str' with `:%s/string/str`\n",
    "\n",
    "And change 'integer' to 'int' with `:%s/integer/int`\n",
    "\n",
    "Clean the units in columns' names with `:%s/(/_` and `:%s/)/` and change the '%' from humidity column name to the word 'Percent'\n",
    "\n",
    "Then copy all the result using your mouse (just easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68028e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that faust is kind of dead. There is a community-maintained fork called faust-streaming\n",
    "!pip install faust-streaming\n",
    "import faust\n",
    "from config import SOURCE_TOPIC, SERVER_PORT, START_EVENTS_TOPIC, END_EVENTS_TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ce85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class row(faust.Record, validation = True):\n",
    "    Event: str \n",
    "    Stream_Time: float \n",
    "    ID: int \n",
    "    Severity: int \n",
    "    Start_Time: str \n",
    "    End_Time: str \n",
    "    Start_Lat: float \n",
    "    Start_Lng: float \n",
    "    End_Lat: float \n",
    "    End_Lng: float \n",
    "    Distance_mi: str \n",
    "    Description: str \n",
    "    Number: int\n",
    "    Street: str \n",
    "    Side: str \n",
    "    City: str \n",
    "    County: str \n",
    "    State: str \n",
    "    Zipcode: str \n",
    "    Country: str \n",
    "    Timezone: str \n",
    "    Airport_Code: str \n",
    "    Weather_Timestamp: str \n",
    "    Temperature_F: float \n",
    "    Wind_Chill_F: float \n",
    "    Humidity_Percent: float \n",
    "    Pressure_in: float\n",
    "    Visibility_mi: float \n",
    "    Wind_Direction: str \n",
    "    Wind_Speed_mph: float \n",
    "    Precipitation_in: float \n",
    "    Weather_Condition: str \n",
    "    Amenity: int \n",
    "    Bump: int\n",
    "    Crossing: int \n",
    "    Give_Way: int \n",
    "    Junction: int \n",
    "    No_Exit: int \n",
    "    Railway: int \n",
    "    Roundabout: int \n",
    "    Station: int \n",
    "    Stop: int \n",
    "    Traffic_Calming: int\n",
    "    Traffic_Signal: int\n",
    "    Turning_Loop: int \n",
    "    Sunrise_Sunset: str \n",
    "    Civil_Twilight: str \n",
    "    Nautical_Twilight: str \n",
    "    Astronomical_Twilight: str "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915b937",
   "metadata": {},
   "source": [
    "In our final versions, we will move this to a separate module. Maybe even create a file with a class that contains the schemas in their different representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa94140",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = faust.App('Divider',broker = f\"kafka://{SERVER_PORT}\")\n",
    "source_topic = app.topic(TOPIC, value_type = row)\n",
    "start_topic = app.topic(START_EVENTS_TOPIC, value_type = row)\n",
    "end_topic = app.topic(END_EVENTS_TOPIC, value_type = row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f43158",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.agent(source_topic)\n",
    "async def end_reading(events):\n",
    "    async for event in events:\n",
    "        if event.Event == 'Start':\n",
    "            await start_event.send(value = event)\n",
    "        else:\n",
    "            await end_event.send(value = event)\n",
    "app.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca919225",
   "metadata": {},
   "source": [
    "You will have to take my word for it when I tell you that this works. But I promise it does.\n",
    "\n",
    "This is the ingestion layer done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18372656",
   "metadata": {},
   "source": [
    "You may ask, why did we split the data? Well, there are two reasons for that.\n",
    "\n",
    "First, I needed an excuse to get into kafka streaming (or faust, you know what I'm talking about) and the source topic is still accessible anyway, so no harm done.\n",
    "\n",
    "Second, in the rare case that we might want to hook a data viz tool directly to kafka (say maybe a live map of total accidents), we will need to disregard the end of an accidents. If we take both start and end events we will end up with double the accidents. Of course we can filter the records in the viz script/tool directly, but just as a personal preference I prefer doing the work in the ingestion layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652bca59",
   "metadata": {},
   "source": [
    "We can also create a Ktable to aggregate the accidents over any column. For instance, we can create a Ktable that has the number of accidents per state. To offload the aggregation from the viz script, but we won't go there now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
