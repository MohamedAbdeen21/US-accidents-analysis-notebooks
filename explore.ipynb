{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c69a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/03 16:07:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"USA Accidents Analysis with Pyspark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3e952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Fedora:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>USA Accidents Analysis with Pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f18d0b41d20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2806c4",
   "metadata": {},
   "source": [
    "Looks like we initialized it successfully\n",
    "Now to reading the data into a spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98454b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " '_c1',\n",
       " '_c2',\n",
       " '_c3',\n",
       " '_c4',\n",
       " '_c5',\n",
       " '_c6',\n",
       " '_c7',\n",
       " '_c8',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '_c12',\n",
       " '_c13',\n",
       " '_c14',\n",
       " '_c15',\n",
       " '_c16',\n",
       " '_c17',\n",
       " '_c18',\n",
       " '_c19',\n",
       " '_c20',\n",
       " '_c21',\n",
       " '_c22',\n",
       " '_c23',\n",
       " '_c24',\n",
       " '_c25',\n",
       " '_c26',\n",
       " '_c27',\n",
       " '_c28',\n",
       " '_c29',\n",
       " '_c30',\n",
       " '_c31',\n",
       " '_c32',\n",
       " '_c33',\n",
       " '_c34',\n",
       " '_c35',\n",
       " '_c36',\n",
       " '_c37',\n",
       " '_c38',\n",
       " '_c39',\n",
       " '_c40',\n",
       " '_c41',\n",
       " '_c42',\n",
       " '_c43',\n",
       " '_c44',\n",
       " '_c45',\n",
       " '_c46']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = spark.read.csv('./Data/US_Accidents_Dec21_updated.csv')\n",
    "main_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d86857",
   "metadata": {},
   "source": [
    "Not quite what we were looking for, let's fetch the headers too and prettify the print with the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2272036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Severity: string (nullable = true)\n",
      " |-- Start_Time: string (nullable = true)\n",
      " |-- End_Time: string (nullable = true)\n",
      " |-- Start_Lat: string (nullable = true)\n",
      " |-- Start_Lng: string (nullable = true)\n",
      " |-- End_Lat: string (nullable = true)\n",
      " |-- End_Lng: string (nullable = true)\n",
      " |-- Distance(mi): string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      " |-- Side: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Timezone: string (nullable = true)\n",
      " |-- Airport_Code: string (nullable = true)\n",
      " |-- Weather_Timestamp: string (nullable = true)\n",
      " |-- Temperature(F): string (nullable = true)\n",
      " |-- Wind_Chill(F): string (nullable = true)\n",
      " |-- Humidity(%): string (nullable = true)\n",
      " |-- Pressure(in): string (nullable = true)\n",
      " |-- Visibility(mi): string (nullable = true)\n",
      " |-- Wind_Direction: string (nullable = true)\n",
      " |-- Wind_Speed(mph): string (nullable = true)\n",
      " |-- Precipitation(in): string (nullable = true)\n",
      " |-- Weather_Condition: string (nullable = true)\n",
      " |-- Amenity: string (nullable = true)\n",
      " |-- Bump: string (nullable = true)\n",
      " |-- Crossing: string (nullable = true)\n",
      " |-- Give_Way: string (nullable = true)\n",
      " |-- Junction: string (nullable = true)\n",
      " |-- No_Exit: string (nullable = true)\n",
      " |-- Railway: string (nullable = true)\n",
      " |-- Roundabout: string (nullable = true)\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Stop: string (nullable = true)\n",
      " |-- Traffic_Calming: string (nullable = true)\n",
      " |-- Traffic_Signal: string (nullable = true)\n",
      " |-- Turning_Loop: string (nullable = true)\n",
      " |-- Sunrise_Sunset: string (nullable = true)\n",
      " |-- Civil_Twilight: string (nullable = true)\n",
      " |-- Nautical_Twilight: string (nullable = true)\n",
      " |-- Astronomical_Twilight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df = spark.read.csv('./Data/US_Accidents_Dec21_updated.csv', header=True)\n",
    "main_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b71f5b",
   "metadata": {},
   "source": [
    "While that worked, Spark inferred that all the columns are strings, even the ID!\n",
    "Let's take a look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8bcef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  ID|\n",
      "+----+\n",
      "| A-1|\n",
      "| A-2|\n",
      "| A-3|\n",
      "| A-4|\n",
      "| A-5|\n",
      "| A-6|\n",
      "| A-7|\n",
      "| A-8|\n",
      "| A-9|\n",
      "|A-10|\n",
      "|A-11|\n",
      "|A-12|\n",
      "|A-13|\n",
      "|A-14|\n",
      "|A-15|\n",
      "|A-16|\n",
      "|A-17|\n",
      "|A-18|\n",
      "|A-19|\n",
      "|A-20|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.select('ID').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d670589",
   "metadata": {},
   "source": [
    "That's a weird choice for an id, we will have to clean that.\n",
    "Let's find some other columns that seem ... suspicious.\n",
    "Using a select with a show method will return columns' names which will make it unreadable. Let's try collect ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f481701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/03 16:07:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ID='A-1', Severity='3', Start_Time='2016-02-08 00:37:08', End_Time='2016-02-08 06:37:08', Start_Lat='40.108909999999995', Start_Lng='-83.09286', End_Lat='40.11206', End_Lng='-83.03187', Distance(mi)='3.23', Description='Between Sawmill Rd/Exit 20 and OH-315/Olentangy Riv Rd/Exit 22 - Accident.', Number=None, Street='Outerbelt E', Side='R', City='Dublin', County='Franklin', State='OH', Zipcode='43017', Country='US', Timezone='US/Eastern', Airport_Code='KOSU', Weather_Timestamp='2016-02-08 00:53:00', Temperature(F)='42.1', Wind_Chill(F)='36.1', Humidity(%)='58.0', Pressure(in)='29.76', Visibility(mi)='10.0', Wind_Direction='SW', Wind_Speed(mph)='10.4', Precipitation(in)='0.0', Weather_Condition='Light Rain', Amenity='False', Bump='False', Crossing='False', Give_Way='False', Junction='False', No_Exit='False', Railway='False', Roundabout='False', Station='False', Stop='False', Traffic_Calming='False', Traffic_Signal='False', Turning_Loop='False', Sunrise_Sunset='Night', Civil_Twilight='Night', Nautical_Twilight='Night', Astronomical_Twilight='Night')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.select('*').limit(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102261a",
   "metadata": {},
   "source": [
    "That's more readable!\n",
    "Timestamps look ok, maybe we will separate dates from times later, but for now this works.\n",
    "It seems like many columns are Boolean type and the last 4 columns take discrete values\n",
    "Let's see what these values are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c38a1b",
   "metadata": {},
   "source": [
    "We already know that the severity column takes values from 1-4 from the kaggle description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c301ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:========================>                                 (5 + 7) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----------------+---------------------+\n",
      "|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|\n",
      "+--------------+--------------+-----------------+---------------------+\n",
      "|         Night|         Night|            Night|                  Day|\n",
      "|          null|          null|             null|                 null|\n",
      "|           Day|           Day|              Day|                  Day|\n",
      "|           Day|         Night|              Day|                  Day|\n",
      "|         Night|         Night|              Day|                  Day|\n",
      "|         Night|           Day|              Day|                  Day|\n",
      "|         Night|           Day|              Day|                Night|\n",
      "|           Day|         Night|              Day|                Night|\n",
      "|         Night|         Night|              Day|                Night|\n",
      "|           Day|           Day|              Day|                Night|\n",
      "|         Night|         Night|            Night|                Night|\n",
      "+--------------+--------------+-----------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "main_df.select('Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88177a1",
   "metadata": {},
   "source": [
    "While this obviously takes the distinct of the combination of all four columns together,\n",
    "not the distinct value of each column, it fulfills our purpose for now.\n",
    "An alternate way was to union each query separately and then call distinct on them\n",
    "unnecessary for this case. \n",
    "\n",
    "Some columns are clearly Boolean. Will need to verify that when loading the data to avoid any surprises.\n",
    "Let's look at other columns for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "263ebe66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:========================>                                 (3 + 4) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "main_df.select(lit(1)).where((main_df.Start_Lng.isNull()) | \n",
    "                      (main_df.Start_Lat.isNull()) |\n",
    "                      (main_df.End_Lng.isNull()) |\n",
    "                       (main_df.End_Lat.isNull())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f274b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+\n",
      "|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|\n",
      "+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+\n",
      "|  False|False|    True|   False|    True|  False|  False|     False|  False|False|          False|          True|       False|\n",
      "|   True|False|   False|   False|   False|   True|  False|     False|  False|False|          False|          True|       False|\n",
      "|  False|False|   False|   False|    True|  False|   True|     False|  False|False|          False|         False|       False|\n",
      "|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|\n",
      "|   True|False|   False|   False|   False|   True|  False|     False|   True|False|          False|         False|       False|\n",
      "|  False|False|    True|   False|   False|  False|  False|     False|   True| True|          False|          True|       False|\n",
      "|   True|False|    True|    True|   False|  False|  False|     False|  False|False|          False|          True|       False|\n",
      "|  False|False|    True|   False|    True|  False|  False|     False|  False| True|          False|          True|       False|\n",
      "|   True|False|   False|   False|   False|  False|  False|     False|   True|False|          False|         False|       False|\n",
      "|  False|False|    True|   False|   False|  False|  False|     False|   True|False|           True|          True|       False|\n",
      "|  False|False|    True|   False|    True|  False|  False|     False|   True|False|          False|          True|       False|\n",
      "|  False|False|    True|   False|   False|  False|  False|     False|   True| True|          False|         False|       False|\n",
      "|   True|False|    True|   False|   False|   True|  False|     False|   True|False|          False|          True|       False|\n",
      "|  False|False|   False|    True|   False|   True|  False|     False|  False|False|          False|          True|       False|\n",
      "|  False|False|   False|   False|   False|  False|  False|     False|  False|False|           True|         False|       False|\n",
      "|  False|False|   False|    True|   False|  False|  False|     False|  False| True|          False|          True|       False|\n",
      "|   True|False|    True|   False|   False|  False|  False|     False|   True|False|          False|         False|       False|\n",
      "|  False|False|   False|   False|   False|  False|   True|     False|  False|False|          False|         False|       False|\n",
      "|   True|False|   False|   False|   False|   True|  False|     False|  False|False|          False|         False|       False|\n",
      "|  False|False|   False|   False|    True|   True|  False|     False|  False|False|          False|         False|       False|\n",
      "+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:============================>                            (6 + 6) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "main_df.select('Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout'\n",
    "                ,'Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fdcf1d",
   "metadata": {},
   "source": [
    "So the boolean columns and the coordinates have no nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.select('Weather_condition').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ebc85",
   "metadata": {},
   "source": [
    "Wow, I wasn't expecting that. Let's see how many unique values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.select('Weather_condition').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da00c98",
   "metadata": {},
   "source": [
    "That's alot, hm ....\n",
    "This will need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc1f6e",
   "metadata": {},
   "source": [
    "From the dataset description on Kaggle, we know that severity is between 1 and 4, we know that all the data should be in the US. We can try to verify that by checking that the coordinates given lie inside the coordinates of the US."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e174e",
   "metadata": {},
   "source": [
    "One last step. We need to know the first start date and the last end date. You will see why in the next script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7f3af",
   "metadata": {},
   "source": [
    "#main_df.agg({\"Start_Time\":\"min\"}).show()\n",
    "#main_df.agg({\"End_Time\":\"max\"}).show()\n",
    "We can use these two above, but I prefer running a SQL query for this one instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.createOrReplaceTempView('main_df')\n",
    "spark.sql('SELECT MIN(Start_Time),MAX(End_Time) FROM main_df').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059023b8",
   "metadata": {},
   "source": [
    "Huh, that end date looks ... odd, is this a default value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.select('*').filter(\"End_Time = '2022-01-01 00:00:00'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07741d2a",
   "metadata": {},
   "source": [
    "No way that road was closed for almost 6 months! Let's use the second latest End time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.select('Start_Time','End_Time').orderBy('End_time', ascending = False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48868e8",
   "metadata": {},
   "source": [
    "The first two seem weird. It's ok, we will just use their end times as nulls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
