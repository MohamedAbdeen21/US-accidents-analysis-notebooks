{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97147a1c",
   "metadata": {},
   "source": [
    "For this project, I decided to use Kafka just to practice connecting Spark with Kafka. Obviously, this project doesn't need Kafka since the data is in a csv. However, we can stream each row based on the start and end time.\n",
    "\n",
    "So, we will make a config.py file that will contain a variable that decides how long should the stream take. In other words, how long should it take to stream the data beginning from the first start date to the last end date to kafka. This variable will be in seconds. \n",
    "\n",
    "Let's try to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764010 sha256=05ab210fdd34481bce5e3e8b88f0f4bdf7031ba351b3aea22c4101514808ccae\n",
      "  Stored in directory: /home/mohamed/.cache/pip/wheels/05/75/73/81f84d174299abca38dd6a06a5b98b08ae25fce50ab8986fa1\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "! pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from config import RUNTIME\n",
    "RUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbcc1e",
   "metadata": {},
   "source": [
    "Now this is out of the way, let's scale the time.\n",
    "\n",
    "From the previous notebook, we know the first start date is '2016-01-14 20:18:33' and the last end date is '2022-01-01 00:00:00'. Let's convert these dates to seconds, scale them to fit inside the RUNTIME window, and stream the entire record (without the end time) with the start time and then stream the end time when its time comes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8610c7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/08 23:13:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/08 23:13:11 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/08/08 23:13:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from config import DATADIR\n",
    "\n",
    "# Using 6 cores instead of * due to the limited memory on my machine, less threads, less memory\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[6]') \\\n",
    "    .appName(\"Transform to Stream With Pyspark\") \\\n",
    "    .config('spark.local.dir',DATADIR + \"/temp/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adef8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3353917",
   "metadata": {},
   "source": [
    "Let's test that and verify using an online tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f50f2498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string, _c27: string, _c28: string, _c29: string, _c30: string, _c31: string, _c32: string, _c33: string, _c34: string, _c35: string, _c36: string, _c37: string, _c38: string, _c39: string, _c40: string, _c41: string, _c42: string, _c43: string, _c44: string, _c45: string, _c46: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = spark.read.csv(DATADIR + 'US_Accidents_Dec21_updated.csv')\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caf37c",
   "metadata": {},
   "source": [
    "The headers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11fe50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Severity: string, Start_Time: string, End_Time: string, Start_Lat: string, Start_Lng: string, End_Lat: string, End_Lng: string, Distance(mi): string, Description: string, Number: string, Street: string, Side: string, City: string, County: string, State: string, Zipcode: string, Country: string, Timezone: string, Airport_Code: string, Weather_Timestamp: string, Temperature(F): string, Wind_Chill(F): string, Humidity(%): string, Pressure(in): string, Visibility(mi): string, Wind_Direction: string, Wind_Speed(mph): string, Precipitation(in): string, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = spark.read.csv(DATADIR + 'US_Accidents_Dec21_updated.csv', header = True)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0950b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Severity: string, Start_Time: string, End_Time: string, Start_Lat: string, Start_Lng: string, End_Lat: string, End_Lng: string, Distance(mi): string, Description: string, Number: string, Street: string, Side: string, City: string, County: string, State: string, Zipcode: string, Country: string, Timezone: string, Airport_Code: string, Weather_Timestamp: string, Temperature(F): string, Wind_Chill(F): string, Humidity(%): string, Pressure(in): string, Visibility(mi): string, Wind_Direction: string, Wind_Speed(mph): string, Precipitation(in): string, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string, Start_Time_Unix: bigint, End_Time_Unix: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType, LongType, datetime\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "main_df = main_df.withColumn('Start_Time_Unix', unix_timestamp(split(main_df['Start_Time'],'\\.').getItem(0)))\n",
    "main_df = main_df.withColumn('End_Time_Unix', unix_timestamp(split(main_df['End_Time'],'\\.').getItem(0)))\n",
    "\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af760ac",
   "metadata": {},
   "source": [
    "We used the split because some of the values in the `Start_Time` and `End_Time` columns had a millisecond. We cut these off with split and remember to escape the `.` with `\\` .\n",
    "\n",
    "Now we're close! Lets create a table with each unix time and a column that stats the id, of course if two time stamps have the same id then the earlier is the start and the later is the end.\n",
    "\n",
    "But first, we need to clean the ID column! This should be easy.\n",
    "\n",
    "Actually let's just set the types appropriately while we at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2821c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------+\n",
      "| ID|Start_Time_Unix|End_Time_Unix|\n",
      "+---+---------------+-------------+\n",
      "|  1|     1454884628|   1454906228|\n",
      "|  2|     1454903780|   1454925380|\n",
      "|  3|     1454904939|   1454926539|\n",
      "|  4|     1454907105|   1454928705|\n",
      "|  5|     1454910823|   1454932423|\n",
      "|  6|     1454912217|   1454933817|\n",
      "|  7|     1454912141|   1454933741|\n",
      "|  8|     1454925106|   1454946706|\n",
      "|  9|     1454933997|   1454955597|\n",
      "| 10|     1454937403|   1454959003|\n",
      "| 11|     1454939030|   1454960630|\n",
      "| 12|     1454943057|   1454964657|\n",
      "| 13|     1454945259|   1454966859|\n",
      "| 14|     1454945418|   1454967018|\n",
      "| 15|     1454947871|   1454969471|\n",
      "| 16|     1454947871|   1454969471|\n",
      "| 17|     1454953662|   1454975262|\n",
      "| 18|     1454953662|   1454975262|\n",
      "| 19|     1454955202|   1454976802|\n",
      "| 20|     1454958017|   1454979617|\n",
      "+---+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Severity: byte (nullable = true)\n",
      " |-- Start_Time: timestamp (nullable = true)\n",
      " |-- End_Time: timestamp (nullable = true)\n",
      " |-- Start_Lat: double (nullable = true)\n",
      " |-- Start_Lng: double (nullable = true)\n",
      " |-- End_Lat: double (nullable = true)\n",
      " |-- End_Lng: double (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Number: integer (nullable = true)\n",
      " |-- Street: string (nullable = false)\n",
      " |-- Side: string (nullable = true)\n",
      " |-- City: string (nullable = false)\n",
      " |-- County: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = false)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Timezone: string (nullable = true)\n",
      " |-- Airport_Code: string (nullable = false)\n",
      " |-- Weather_Timestamp: timestamp (nullable = true)\n",
      " |-- Wind_Direction: string (nullable = false)\n",
      " |-- Weather_Condition: string (nullable = false)\n",
      " |-- Amenity: boolean (nullable = true)\n",
      " |-- Bump: boolean (nullable = true)\n",
      " |-- Crossing: boolean (nullable = true)\n",
      " |-- Give_Way: boolean (nullable = true)\n",
      " |-- Junction: boolean (nullable = true)\n",
      " |-- No_Exit: boolean (nullable = true)\n",
      " |-- Railway: boolean (nullable = true)\n",
      " |-- Roundabout: boolean (nullable = true)\n",
      " |-- Station: boolean (nullable = true)\n",
      " |-- Stop: boolean (nullable = true)\n",
      " |-- Traffic_Calming: boolean (nullable = true)\n",
      " |-- Traffic_Signal: boolean (nullable = true)\n",
      " |-- Turning_Loop: boolean (nullable = true)\n",
      " |-- Sunrise_Sunset: string (nullable = false)\n",
      " |-- Civil_Twilight: string (nullable = false)\n",
      " |-- Nautical_Twilight: string (nullable = false)\n",
      " |-- Astronomical_Twilight: string (nullable = false)\n",
      " |-- Start_Time_Unix: long (nullable = true)\n",
      " |-- End_Time_Unix: long (nullable = true)\n",
      " |-- Precipitation_in: float (nullable = true)\n",
      " |-- Wind_Speed_mph: float (nullable = true)\n",
      " |-- Temperature_F: float (nullable = true)\n",
      " |-- Wind_Chill_F: float (nullable = true)\n",
      " |-- Pressure_in: float (nullable = true)\n",
      " |-- Humidity_Percent: float (nullable = true)\n",
      " |-- Visibility_mi: float (nullable = true)\n",
      " |-- Distance_mi: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import TimestampType, ByteType, IntegerType, FloatType, DoubleType, BooleanType\n",
    "\n",
    "bool_columns = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout'\n",
    "                ,'Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop']\n",
    "\n",
    "for column in bool_columns:\n",
    "    main_df = main_df.withColumn(column, main_df[column].cast(BooleanType()))\n",
    "\n",
    "float_columns = ['Precipitation(in)','Wind_Speed(mph)' , 'Temperature(F)', 'Wind_Chill(F)'\n",
    "                , 'Pressure(in)','Humidity(%)' , 'Visibility(mi)']\n",
    "\n",
    "float_columns_renamed = ['Precipitation_in','Wind_Speed_mph' , 'Temperature_F', 'Wind_Chill_F'\n",
    "                , 'Pressure_in','Humidity_Percent' , 'Visibility_mi']\n",
    "\n",
    "# Other columns don't have nulls or we prefer them to stay nulls \n",
    "main_df = main_df.fillna('-1', subset = float_columns + ['Number', 'Zipcode'])\n",
    "\n",
    "for column,renamed in zip(float_columns, float_columns_renamed):\n",
    "    main_df = main_df.withColumn(renamed, main_df[column].cast(FloatType()))\n",
    "    \n",
    "string_nulls = ['Wind_Direction','Weather_Condition','Airport_Code', 'City', 'Street' \n",
    "               ,'Astronomical_Twilight', 'Nautical_Twilight', 'Civil_Twilight', 'Sunrise_Sunset']\n",
    "\n",
    "main_df = main_df.fillna('Not Available', subset = string_nulls)\n",
    "main_df = main_df.fillna('22022-01-01 00:00:00', subset = 'Weather_Timestamp')\n",
    "\n",
    "main_df = main_df.drop(*float_columns)\n",
    "\n",
    "main_df = main_df.withColumn('ID', split(main_df['ID'],'-').getItem(1).cast(IntegerType()))\n",
    "main_df = (main_df.withColumn('Severity', main_df['Severity'].cast(ByteType())) \n",
    "                .withColumn('Start_Time', main_df['Start_Time'].cast(TimestampType())) \n",
    "                .withColumn('End_Time', main_df['End_Time'].cast(TimestampType())) \n",
    "                .withColumn('Start_Lat', main_df['Start_Lat'].cast(DoubleType())) \n",
    "                .withColumn('End_Lat', main_df['End_Lat'].cast(DoubleType())) \n",
    "                .withColumn('Start_Lng', main_df['Start_Lng'].cast(DoubleType()))  \n",
    "                .withColumn('End_Lng', main_df['End_Lng'].cast(DoubleType()))  \n",
    "                .withColumn('Distance_mi', main_df['Distance(mi)'].cast(DoubleType()))\n",
    "                .drop('Distance(mi)')\n",
    "                .withColumn('Number', main_df['Number'].cast(IntegerType()))\n",
    "                .withColumn('Weather_Timestamp', main_df['Weather_Timestamp'].cast(TimestampType())) \\\n",
    "          )\n",
    "\n",
    "\n",
    "main_df.select('ID','Start_Time_Unix','End_Time_Unix').show()\n",
    "main_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273c718",
   "metadata": {},
   "source": [
    "I know someone might ask, but why set the types? Well, if any of the following services in our project expect a type different than the actual type, it can cause an error or return nulls.\n",
    "\n",
    "In the following notebooks, I'll try to point out each point where not changing the type can cause a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc05e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+\n",
      "|Event| ID|Start_Time_Unix|\n",
      "+-----+---+---------------+\n",
      "|Start|  1|     1454884628|\n",
      "|  End|  1|     1454906228|\n",
      "|  End|  2|     1454925380|\n",
      "|Start|  2|     1454903780|\n",
      "|  End|  3|     1454926539|\n",
      "|Start|  3|     1454904939|\n",
      "|Start|  4|     1454907105|\n",
      "|  End|  4|     1454928705|\n",
      "|Start|  5|     1454910823|\n",
      "|  End|  5|     1454932423|\n",
      "|  End|  6|     1454933817|\n",
      "|Start|  6|     1454912217|\n",
      "|Start|  7|     1454912141|\n",
      "|  End|  7|     1454933741|\n",
      "|Start|  8|     1454925106|\n",
      "|  End|  8|     1454946706|\n",
      "|  End|  9|     1454955597|\n",
      "|Start|  9|     1454933997|\n",
      "|  End| 10|     1454959003|\n",
      "|Start| 10|     1454937403|\n",
      "+-----+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Event: string (nullable = false)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Start_Time_Unix: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:=====================================================>   (17 + 1) / 18]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "temp_df = main_df.select(lit('Start'),'ID','Start_Time_Unix') \\\n",
    "        .union(main_df.select(lit('End'),'ID','End_Time_Unix')) \\\n",
    "        .withColumnRenamed('Start', 'Event') \\\n",
    "        .orderBy('Start_Time_Unix') \\\n",
    "        .orderBy('ID')\n",
    "temp_df.show()\n",
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb2e08",
   "metadata": {},
   "source": [
    "Absolute perfection! Maybe change column name, but other than that ... absolute perfection!\n",
    "\n",
    "You may notice that the second column is basically the same value repeated, but this is multiplied by 10^9.\n",
    "\n",
    "So, all this work and we still didn't get the scaling done. The scaling after this point is easy, we just subtract the earliest time and divide by the latest time and multiply by the RUNTIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e454d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(min(Time_Unix)=1452795513)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = temp_df.withColumn('Time_Unix',temp_df['Start_Time_Unix'])\n",
    "earliest = temp_df.agg({'Time_Unix':\"min\"}).collect()\n",
    "earliest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71b766",
   "metadata": {},
   "source": [
    "The value is inside a Pyspark row. Don't worry, we can get it out.\n",
    "A simple google search leads us to a few methods to do so, below is two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181687ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1452795513"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# earliest = earliest[0].__getitem__('min(Time_Unix)')\n",
    "earliest = earliest[0][0]\n",
    "earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e311640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1640988000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = temp_df.agg({\"Time_Unix\":\"max\"}).collect()\n",
    "latest = latest[0][0]\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f505192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify the return type of the udf, instead of the approach we used before\n",
    "temp_df = temp_df.withColumn('Stream_Time',\n",
    "            (temp_df['Time_Unix'].cast(FloatType()) - earliest) * RUNTIME / (latest - earliest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a923a9",
   "metadata": {},
   "source": [
    "A job well done!\n",
    "Just one final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c3e2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.withColumnRenamed('ID','temp_id') # so we can drop it\n",
    "to_delete = ('Start_Time_Unix','End_Time_Unix','Time_Unix',\"temp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663db3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = temp_df.join(main_df, temp_df.temp_id == main_df.ID)\n",
    "stream_df = stream_df.drop(*to_delete)\n",
    "stream_df = stream_df.orderBy('Stream_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "613f17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5690684"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b92e8f",
   "metadata": {},
   "source": [
    "That was alot of work. Now we will just write to a parquet file and read from it in the streaming script. First, we will repartition the dataframe. Why? Because we don't want to run out of memory and more partitions means each partition is smaller (and more balanced) and less memory pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c427d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/08 23:13:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|SPARK_PARTITION_ID()|  count|\n",
      "+--------------------+-------+\n",
      "|                   0| 840255|\n",
      "|                   1| 919366|\n",
      "|                   2| 929988|\n",
      "|                   3| 898228|\n",
      "|                   4| 951716|\n",
      "|                   5|1151131|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   0|113814|\n",
      "|                   1|113814|\n",
      "|                   2|113814|\n",
      "|                   3|113814|\n",
      "|                   4|113814|\n",
      "|                   5|113814|\n",
      "|                   6|113814|\n",
      "|                   7|113814|\n",
      "|                   8|113814|\n",
      "|                   9|113815|\n",
      "|                  10|113814|\n",
      "|                  11|113814|\n",
      "|                  12|113814|\n",
      "|                  13|113815|\n",
      "|                  14|113815|\n",
      "|                  15|113815|\n",
      "|                  16|113815|\n",
      "|                  17|113815|\n",
      "|                  18|113815|\n",
      "|                  19|113814|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "stream_df.groupBy(spark_partition_id()).count().show()\n",
    "stream_df = stream_df.repartition(50)\n",
    "stream_df.groupBy(spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706167e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/08 23:14:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "targetfolder = DATADIR + 'parquet/'\n",
    "stream_df.coalesce(1).write.parquet(targetfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff30e2e",
   "metadata": {},
   "source": [
    "What is coalesce? Basically I needed to write out to a parquet file, removing the coalesce(1) from the last line will give us a folder with multiple parquet files inside; that is a file for each partition.\n",
    "\n",
    "To ask pyspark for a single file we need to reduce the number of partitions using repartition or coalesce. They both can reduce number of partitions; however, coalesce is more optimized since it can only reduce number of partitions (unlike repartition). \n",
    "\n",
    "Now to rename the output file so that we can read it from the streaming script without this weird name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(targetfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443c495",
   "metadata": {},
   "source": [
    "What even is that name, spark? It's ok, we got it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir(targetfolder)[0]\n",
    "os.rename(f'{targetfolder}{filename}', f'{targetfolder}stream_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e1509",
   "metadata": {},
   "source": [
    "And we're done for this one. Let's now stream that dataframe to a kafka topic. Woohoo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
