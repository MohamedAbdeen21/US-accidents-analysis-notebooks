{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96661f9e",
   "metadata": {},
   "source": [
    "The first part is the exact same as the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7411351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import *\n",
    "from config import SOURCE_TOPIC, SERVER_PORT, DATADIR\n",
    "\n",
    "spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .master(\"local[6]\") \\\n",
    "          .appName(\"Kafka to postgres\") \\\n",
    "          .getOrCreate()\n",
    "\n",
    "df = spark\\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", SERVER_PORT) \\\n",
    "      .option(\"subscribe\", SOURCE_TOPIC) \\\n",
    "      .option(\"startingOffsets\", \"latest\") \\\n",
    "      .option('failOnDataLoss','false') \\\n",
    "      .load() \\\n",
    "      .select('value','timestamp')\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('Event',StringType() , False),\n",
    "    StructField('Stream_Time', DoubleType() , False),\n",
    "    StructField('ID', IntegerType() , False),\n",
    "    StructField('Severity',ByteType() , False),\n",
    "    StructField('Start_Time',TimestampType() , False),\n",
    "    StructField('End_Time',TimestampType() , False),\n",
    "    StructField('Start_Lat',DoubleType() , False),\n",
    "    StructField('Start_Lng',DoubleType() , False),\n",
    "    StructField('End_Lat',DoubleType() , False),\n",
    "    StructField('End_Lng',DoubleType() , False),\n",
    "    StructField('Distance_mi',DoubleType() , False),\n",
    "    StructField('Description',StringType() , False),\n",
    "    StructField('Number',StringType() , False),\n",
    "    StructField('Street',StringType() , False),\n",
    "    StructField('Side',StringType() , False),\n",
    "    StructField('City',StringType() , False),\n",
    "    StructField('County',StringType() , False),\n",
    "    StructField('State',StringType() , False),\n",
    "    StructField('Zipcode',StringType() , False),\n",
    "    StructField('Country',StringType() , False),\n",
    "    StructField('Timezone',StringType() , False),\n",
    "    StructField('Airport_Code',StringType() , False),\n",
    "    StructField('Weather_Timestamp',TimestampType() , False),\n",
    "    StructField('Temperature_F',DoubleType() , False),\n",
    "    StructField('Wind_Chill_F',DoubleType() , False),\n",
    "    StructField('Humidity_Percent',DoubleType() , False),\n",
    "    StructField('Pressure_in',DoubleType() , False),\n",
    "    StructField('Visibility_mi',DoubleType() , False),\n",
    "    StructField('Wind_Direction',StringType() , False),\n",
    "    StructField('Wind_Speed_mph',DoubleType() , False),\n",
    "    StructField('Precipitation_in',DoubleType() , False),\n",
    "    StructField('Weather_Condition',StringType() , False),\n",
    "    StructField('Amenity',BooleanType() , False),\n",
    "    StructField('Bump',BooleanType() , False),\n",
    "    StructField('Crossing',BooleanType() , False),\n",
    "    StructField('Give_Way',BooleanType() , False),\n",
    "    StructField('Junction',BooleanType() , False),\n",
    "    StructField('No_Exit',BooleanType() , False),\n",
    "    StructField('Railway',BooleanType() , False),\n",
    "    StructField('Roundabout',BooleanType() , False),\n",
    "    StructField('Station',BooleanType() , False),\n",
    "    StructField('Stop',BooleanType() , False),\n",
    "    StructField('Traffic_Calming',BooleanType() , False),\n",
    "    StructField('Traffic_Signal',BooleanType() , False),\n",
    "    StructField('Turning_Loop',BooleanType() , False),\n",
    "    StructField('Sunrise_Sunset',StringType() , False),\n",
    "    StructField('Civil_Twilight',StringType() , False),\n",
    "    StructField('Nautical_Twilight',StringType() , False),\n",
    "    StructField('Astronomical_Twilight',StringType(), False)\n",
    "    ])\n",
    "\n",
    "df = df \\\n",
    "    .withColumn('json', df.value.cast(StringType())) \\\n",
    "    .withColumn('jsonData', from_json(col('json'), schema)) \\\n",
    "    .drop('json','value')\n",
    "\n",
    "new_df = df \\\n",
    "        .select('jsonData.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c7e55",
   "metadata": {},
   "source": [
    "Pyspark-postgreSQL jar don't support streaming. However, pyspark supports passing functions to `foreachBatch` method (there is also `foreach` method). \n",
    "\n",
    "To run the `.py` from the terminal, we change the command a bit. We instead use:\n",
    "`spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 --driver-class-path Data/postgresql-42.4.1.jar kafka_to_psql.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send(df, batchId):\n",
    "    df.write.format('jdbc') \\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/stream_df\") \\\n",
    "        .option(\"dbtable\", \"df\") \\\n",
    "        .option(\"user\", \"user\") \\\n",
    "        .option(\"password\", \"root\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode('append') \\\n",
    "        .save()\n",
    "    pass\n",
    "\n",
    "query = new_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode('append') \\\n",
    "        .foreachBatch(send) \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b888c8",
   "metadata": {},
   "source": [
    "This works flawlessly; however, you will notice that the database has no schema. This isn't even a star schema warehouse! I'm still working on the Star Schema design and how to write to it with Pyspark. Once I finish it I'll be updating the main repo and this repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13bdbb9",
   "metadata": {},
   "source": [
    "Now, we install superset, read the data, and create some charts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
