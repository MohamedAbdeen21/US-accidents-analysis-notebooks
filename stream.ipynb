{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b97539",
   "metadata": {},
   "source": [
    "When it comes to projects, I'm a firm believer that you should put yourself in weird situations, maybe you run into something similar while working one day, so just wing it.\n",
    "\n",
    "Now that we have the parquet file, I have no idea how to stream that. Pyspark can stream directly to kafka, however it does so with no regards to time column. So ... maybe use a database and run a query like ```SELECT * FROM dataframe WHERE Stream_Time BETWEEN last_run and NOW()``` , where last_run is some variable that has the value of `NOW()` from the last run. And have repeat this query untill last_run is >= the RUNTUME.\n",
    "\n",
    "Sound like a good idea. \n",
    "\n",
    "How should we put that parquet to a database? Well, Pyspark can write to a JDBC (if you don't know what these are, think of it like a library that allows connecting java application to databases). However, it kept raising an out of memory error. Therefore, we will do them the old way.\n",
    "\n",
    "Pandas has a way to write directly to sqlite3 database, let's try that and see if the performance is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4de5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_parquet('./parquet/stream_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206845a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "conn = sqlite3.connect('./stream_df.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637f0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_sql(name='stream', con=conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b2070",
   "metadata": {},
   "source": [
    "That's acceptable. Now let's start a kafka producer and start the stream. The kafka cluster is already up and running. [Kafka](https://kafka.apache.org/quickstart)'s official website provides a quickstart guide on how to start the cluster and create a topic. Those steps will be enough for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c675329",
   "metadata": {},
   "source": [
    "But before doing that we need to test the speed of our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922253e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %timeit cur.execute('SELECT * FROM stream WHERE Stream_Time BETWEEN 0 AND 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78889b4e",
   "metadata": {},
   "source": [
    "pretty fast. Now let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db18ffbc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m cur\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM stream WHERE Stream_Time BETWEEN ? AND ?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m                 ,(last_run, now))\n\u001b[1;32m     17\u001b[0m last_run \u001b[38;5;241m=\u001b[39m now\n\u001b[0;32m---> 18\u001b[0m data \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m!=\u001b[39m []:\n\u001b[1;32m     20\u001b[0m     producer\u001b[38;5;241m.\u001b[39msend(TOPIC,data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "from config import TOPIC, SERVER_PORT, RUNTIME\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def serializer(string):\n",
    "    return pickle.dumps(string)\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[SERVER_PORT], value_serializer = serializer)\n",
    "start_time = time.time()\n",
    "last_run = 0\n",
    "total = 0\n",
    "while last_run <= RUNTIME:\n",
    "    now = time.time() - start_time\n",
    "    cur.execute('SELECT * FROM stream WHERE Stream_Time BETWEEN ? AND ?'\n",
    "                    ,(last_run, now))\n",
    "    last_run = now\n",
    "    response = cur.fetchall()\n",
    "    if response != []:\n",
    "        producer.send(TOPIC,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fe38d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
