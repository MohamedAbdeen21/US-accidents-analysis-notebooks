{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b97539",
   "metadata": {},
   "source": [
    "When it comes to projects, I'm a firm believer that you should put yourself in weird situations, maybe you run into something similar while working one day, so just wing it.\n",
    "\n",
    "Now that we have the parquet file, I have no idea how to stream that. Pyspark can stream directly to kafka, however it does so with no regards to time column. So ... maybe use a database and run a query like ```SELECT * FROM dataframe WHERE Stream_Time BETWEEN last_run and NOW()``` , where last_run is some variable that has the value of `NOW()` from the last run. And have repeat this query untill `last_run > RUNTUME`.\n",
    "\n",
    "Sound like a good idea. \n",
    "\n",
    "How should we put that parquet to a database? Well, Pyspark can write to a JDBC (if you don't know what these are, think of it like a library that allows connecting java application to databases). However, sqlite3 doesn't work well with pyspark, it has a JDBC, but still not that great.\n",
    "\n",
    "Pandas has a way to write directly to sqlite3 database, let's try that and see if the performance is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4de5390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 s ± 214 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%timeit pd.read_parquet('./parquet/stream_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6986c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('./parquet/stream_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206845a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "conn = sqlite3.connect('./stream_df.db')\n",
    "# Let sqlite return a list of dictionaries instead of list of tuples\n",
    "conn.row_factory = sqlite3.Row \n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "637f0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(name='stream', con=conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b2070",
   "metadata": {},
   "source": [
    "That's acceptable. Now let's start a kafka producer and start the stream. The kafka cluster is already up and running. [Kafka](https://kafka.apache.org/quickstart)'s official website provides a quickstart guide on how to start the cluster and create a topic. Those steps will be enough for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c675329",
   "metadata": {},
   "source": [
    "But before doing that we need to test the speed of our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922253e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 µs ± 277 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cur.execute('SELECT * FROM stream WHERE Stream_Time BETWEEN 0 AND 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78889b4e",
   "metadata": {},
   "source": [
    "pretty fast. Now let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db18ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from config import SOURCE_TOPIC, SERVER_PORT, RUNTIME\n",
    "import time\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[SERVER_PORT],\n",
    "                         value_serializer = lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "start_time = time.time()\n",
    "last_run = 0\n",
    "while last_run <= RUNTIME:\n",
    "    now = time.time() - start_time\n",
    "    cur.execute('SELECT * FROM stream WHERE Stream_Time BETWEEN ? AND ?'\n",
    "                    ,(last_run, now))\n",
    "    last_run = now\n",
    "    # sqlite3 returns sqlite3.Row objects, need to call dict() on each object\n",
    "    response = [dict(row) for row in cur.fetchall()] \n",
    "    if response != []:\n",
    "        producer.send(topic = SOURCE_TOPIC, value = response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ff376",
   "metadata": {},
   "source": [
    "Our kafka console consumer recieved the data. Our job here is done and can safely ctrl-c.\n",
    "\n",
    "We sent `value` without a `key` because keys are just to group events with the same key in the same partition to ensure order (useful for keeping states). However, we don't really care about order of the events.\n",
    "\n",
    "The next step is writing a kafka streams job to split the data into to other topics, one that includes the beginning of an event and the other contains the end of an event.\n",
    "\n",
    "This concludes the streaming/ingestion layer. We will finalize the ingestion with the kafka streams in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
