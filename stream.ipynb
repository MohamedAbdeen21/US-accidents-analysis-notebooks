{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b97539",
   "metadata": {},
   "source": [
    "When it comes to projects, I'm a firm believer that you should put yourself in weird situations, maybe you run into something similar while working one day, so just wing it.\n",
    "\n",
    "Now that we have the parquet file, I have no idea how to stream that. Pyspark can stream directly to kafka, however it does so with no regards to time column. So ... maybe use a database and run a query like ```SELECT * FROM dataframe WHERE Stream_Time BETWEEN last_run and NOW()``` , where last_run is some variable that has the value of `NOW()` from the last run. And have repeat this query untill `last_run > RUNTUME`.\n",
    "\n",
    "Sound like a good idea. \n",
    "\n",
    "How should we put that parquet to a database? Well, Pyspark can write to a JDBC (if you don't know what these are, think of it like a library that allows connecting java application to databases). However, sqlite3 doesn't work well with pyspark, it has a JDBC, but still not that great.\n",
    "\n",
    "Pandas has a way to write directly to sqlite3 database, let's try that and see if the performance is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4de5390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.61 s ± 131 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%timeit pd.read_parquet('./Data/parquet/stream_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6986c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5690684 entries, 0 to 5690683\n",
      "Data columns (total 49 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   Start                  object        \n",
      " 1   Stream_Time            float64       \n",
      " 2   ID                     int32         \n",
      " 3   Severity               int8          \n",
      " 4   Start_Time             datetime64[ns]\n",
      " 5   End_Time               datetime64[ns]\n",
      " 6   Start_Lat              float64       \n",
      " 7   Start_Lng              float64       \n",
      " 8   End_Lat                float64       \n",
      " 9   End_Lng                float64       \n",
      " 10  Distance(mi)           float64       \n",
      " 11  Description            object        \n",
      " 12  Number                 float32       \n",
      " 13  Street                 object        \n",
      " 14  Side                   object        \n",
      " 15  City                   object        \n",
      " 16  County                 object        \n",
      " 17  State                  object        \n",
      " 18  Zipcode                object        \n",
      " 19  Country                object        \n",
      " 20  Timezone               object        \n",
      " 21  Airport_Code           object        \n",
      " 22  Weather_Timestamp      datetime64[ns]\n",
      " 23  Temperature(F)         float32       \n",
      " 24  Wind_Chill(F)          float32       \n",
      " 25  Humidity(%)            float32       \n",
      " 26  Pressure(in)           float32       \n",
      " 27  Visibility(mi)         float32       \n",
      " 28  Wind_Direction         object        \n",
      " 29  Wind_Speed(mph)        float32       \n",
      " 30  Precipitation(in)      float32       \n",
      " 31  Weather_Condition      object        \n",
      " 32  Amenity                bool          \n",
      " 33  Bump                   bool          \n",
      " 34  Crossing               bool          \n",
      " 35  Give_Way               bool          \n",
      " 36  Junction               bool          \n",
      " 37  No_Exit                bool          \n",
      " 38  Railway                bool          \n",
      " 39  Roundabout             bool          \n",
      " 40  Station                bool          \n",
      " 41  Stop                   bool          \n",
      " 42  Traffic_Calming        bool          \n",
      " 43  Traffic_Signal         bool          \n",
      " 44  Turning_Loop           bool          \n",
      " 45  Sunrise_Sunset         object        \n",
      " 46  Civil_Twilight         object        \n",
      " 47  Nautical_Twilight      object        \n",
      " 48  Astronomical_Twilight  object        \n",
      "dtypes: bool(13), datetime64[ns](3), float32(8), float64(6), int32(1), int8(1), object(17)\n",
      "memory usage: 1.4+ GB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('./Data/parquet/stream_df.parquet')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206845a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "conn = sqlite3.connect('./Data/stream_df.db')\n",
    "# Let sqlite return a list of dictionaries instead of list of tuples\n",
    "conn.row_factory = sqlite3.Row \n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "637f0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(name = 'stream', con = conn, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b2070",
   "metadata": {},
   "source": [
    "That's acceptable. Now let's start a kafka producer and start the stream. The kafka cluster is already up and running. [Kafka](https://kafka.apache.org/quickstart)'s official website provides a quickstart guide on how to start the cluster and create a topic. Those steps will be enough for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c675329",
   "metadata": {},
   "source": [
    "But before doing that we need to test the speed of our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922253e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 µs ± 572 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cur.execute('SELECT * FROM stream WHERE Stream_Time BETWEEN 0 AND 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78889b4e",
   "metadata": {},
   "source": [
    "pretty fast. Now let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from config import SOURCE_TOPIC, SERVER_PORT, RUNTIME\n",
    "import time\n",
    "import json\n",
    "import sqlite3 \n",
    "conn = sqlite3.connect('./Data/stream_df.db')\n",
    "# Let sqlite return a list of dictionaries instead of list of tuples\n",
    "conn.row_factory = sqlite3.Row \n",
    "cur = conn.cursor()\n",
    "producer = KafkaProducer(bootstrap_servers=[SERVER_PORT],\n",
    "                         value_serializer = lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "start_time = time.time()\n",
    "last_run = 0\n",
    "while last_run <= RUNTIME:\n",
    "    now = time.time() - start_time\n",
    "    cur.execute('SELECT * FROM stream WHERE Stream_Time BETWEEN ? AND ?'\n",
    "                    ,(last_run, now))\n",
    "    last_run = now\n",
    "    \n",
    "    # sqlite3 returns sqlite3.Row objects, need to call dict() on each object\n",
    "    response = [dict(row) for row in cur.fetchall()] \n",
    "    if response != []:\n",
    "        producer.send(topic = SOURCE_TOPIC, value = response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ff376",
   "metadata": {},
   "source": [
    "Our kafka console consumer recieved the data. Our job here is done and can safely ctrl-c.\n",
    "\n",
    "We sent `value` without a `key` because keys are just to group events with the same key in the same partition to ensure order (useful for keeping states). However, we don't really care about order of the events.\n",
    "\n",
    "The next step is writing a kafka streams job to split the data into to other topics, one that includes the beginning of an event and the other contains the end of an event.\n",
    "\n",
    "This concludes the streaming/ingestion layer. We will finalize the ingestion with the kafka streams in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
