{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b97539",
   "metadata": {},
   "source": [
    "When it comes to projects, I'm a firm believer that you should put yourself in weird situations, maybe you run into something similar while working one day, so just wing it.\n",
    "\n",
    "Now that we have the parquet file, I have no idea how to stream that. Pyspark can stream directly to kafka, however it does so with no regards to time column. So ... maybe use a database and run a query like ```SELECT * FROM dataframe WHERE Stream_Time BETWEEN last_run and NOW()``` , where last_run is some variable that has the value of `NOW()` from the last run. And have repeat this query untill `last_run > RUNTUME`.\n",
    "\n",
    "Sound like a good idea. \n",
    "\n",
    "How should we put that parquet to a database? Well, Pyspark can write to a JDBC (if you don't know what these are, think of it like a library that allows connecting java application to databases). However, sqlite3 doesn't work well with pyspark, it has a JDBC, but still not that great.\n",
    "\n",
    "Pandas has a way to write directly to sqlite3 database, let's try that and see if the performance is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95512610",
   "metadata": {},
   "source": [
    "Parquet preserves datatypes, but let's double check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353222a",
   "metadata": {},
   "source": [
    "Now, to write to our database, to write a schema to a `sqlite3` database we need to use `sqlalchemy.engine` object, because we will use `sqlalchemy.types` to define the schema. However, for querying the database I much prefer `sqlite3` standard driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318be95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "conn = create_engine('sqlite:///./Data/stream_df.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d70d2",
   "metadata": {},
   "source": [
    "We will need to pass our schema to `to_sql` method. However, the `schema` parameter means something else in the `to_sql` function. The 'schema' we are looking for is passed through `dtype` parameter.\n",
    "\n",
    "Of course I didn't write that by hand, I used some ✨ vim magic ✨ (It's regex magic, but I used vim for it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776fc19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy.types as T\n",
    "dtypes = {'Event':T.String(),\n",
    "            'Stream_Time':T.Float(asdecimal=True),\n",
    "            'ID':T.Integer(),\n",
    "            'Severity':T.SmallInteger(),\n",
    "            'Start_Time':T.DateTime(),\n",
    "            'End_Time':T.DateTime(),\n",
    "            'Start_Lat':T.Float(asdecimal=True),\n",
    "            'Start_Lng':T.Float(asdecimal=True),\n",
    "            'End_Lat':T.Float(asdecimal=True),\n",
    "            'End_Lng':T.Float(asdecimal=True),\n",
    "            'Distance_mi':T.Float(asdecimal=True),\n",
    "            'Description':T.String(),\n",
    "            'Number':T.Integer,\n",
    "            'Street':T.String(),\n",
    "            'Side':T.String(),\n",
    "            'City':T.String(),\n",
    "            'County':T.String(),\n",
    "            'State':T.String(),\n",
    "            'Zipcode':T.String(),\n",
    "            'Country':T.String(),\n",
    "            'Timezone':T.String(),\n",
    "            'Airport_Code':T.String(),\n",
    "            'Weather_Timestamp':T.DateTime(),\n",
    "            'Temperature_F':T.Float(asdecimal=True),\n",
    "            'Wind_Chill_F':T.Float(asdecimal=True),\n",
    "            'Humidity_Percent':T.Float(asdecimal=True),\n",
    "            'Pressure_in':T.Float(asdecimal=True),\n",
    "            'Visibility_m':T.Float(asdecimal=True),\n",
    "            'Wind_Direction':T.String(),\n",
    "            'Wind_Speed_mph':T.Float(asdecimal=True),\n",
    "            'Precipitation_in':T.Float(asdecimal=True),\n",
    "            'Weather_Condition':T.String(),\n",
    "            'Amenity':T.Boolean,\n",
    "            'Bump':T.Boolean,\n",
    "            'Crossing':T.Boolean,\n",
    "            'Give_Way':T.Boolean,\n",
    "            'Junction':T.Boolean,\n",
    "            'No_Exit':T.Boolean,\n",
    "            'Railway':T.Boolean,\n",
    "            'Roundabout':T.Boolean,\n",
    "            'Station':T.Boolean,\n",
    "            'Stop':T.Boolean,\n",
    "            'Traffic_Calming':T.Boolean,\n",
    "            'Traffic_Signal':T.Boolean,\n",
    "            'Turning_Loop':T.Boolean,\n",
    "            'Sunrise_Sunset':T.String(),\n",
    "            'Civil_Twilight':T.String(),\n",
    "            'Nautical_Twilight':T.String(),\n",
    "            'Astronomical_Twilight':T.String()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd9ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 32s, sys: 13.2 s, total: 3min 45s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('./Data/parquet/stream_df.parquet')\n",
    "%time df.to_sql(name = 'stream', con = conn.engine, if_exists = 'replace', dtype = dtypes, \\\n",
    "                chunksize = 100000, index = False)\n",
    "conn.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79e61b",
   "metadata": {},
   "source": [
    "That's way too slow. Let's try dropping the schema and using sqlite3 connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f307a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "\n",
    "conn = sqlite3.connect('./Data/stream_df.db')\n",
    "# Let sqlite return a list of dictionaries instead of list of tuples\n",
    "conn.row_factory = sqlite3.Row \n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283f163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 13.6 s, total: 1min 45s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%time df.to_sql(name = 'stream', con = conn, if_exists = 'replace', chunksize = 100000, \\\n",
    "               index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b2070",
   "metadata": {},
   "source": [
    "That's less than half the time!\n",
    "\n",
    "If that is faster, why even bother with the schema? Well, generally in databases, you should always define your data types appropriately. Using appropriate data types is usually better in storage space and in performance; it helps the RDBMS decide the best plan to query the data, helps you optimize indexes if you have any, and helps optimize space.\n",
    "\n",
    "One thing to notice, `sqlite3` will try to infer the schema, and it incorrectly guesses the boolean columns (it saves them as Integers of zeroes and ones); however, it stores them as 0s and 1s. Therefore, when we try to read them especially with spark, we will have to declare their types as integers. Because spark expects values of `True` and `False` when you say boolean.\n",
    "\n",
    "Let us test the speed of our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6e6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 µs ± 2.03 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cur.execute('SELECT * FROM stream WHERE Stream_Time BETWEEN 0 AND 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f5aeb9",
   "metadata": {},
   "source": [
    "That's pretty good. Now let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd946c9",
   "metadata": {},
   "source": [
    "Now let's start a kafka producer and start the stream. The kafka cluster is already up and running. [Kafka](https://kafka.apache.org/quickstart)'s official website provides a quickstart guide on how to start the cluster and create a topic. Those steps will be enough for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db18ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from config import SOURCE_TOPIC, SERVER_PORT, RUNTIME\n",
    "import time\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[SERVER_PORT],\n",
    "                         value_serializer = lambda x: json.dumps(x, indent = 4).encode('utf-8'))\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    last_run = 0\n",
    "    while last_run <= RUNTIME:\n",
    "        now = time.time() - start_time\n",
    "        cur.execute('SELECT * FROM stream WHERE Stream_Time BETWEEN ? AND ?'\n",
    "                        ,(last_run, now))\n",
    "        last_run = now\n",
    "\n",
    "        # sqlite3 returns sqlite3.Row objects, need to call dict() on each object\n",
    "        response = [dict(row) for row in cur.fetchall()] \n",
    "        if response != []:\n",
    "            for jsonObject in response:\n",
    "                producer.send(topic = SOURCE_TOPIC, value = jsonObject)\n",
    "except KeyboardInterrupt:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ff376",
   "metadata": {},
   "source": [
    "Our kafka console consumer recieved the data. Our job here is done and can safely ctrl-c.\n",
    "\n",
    "We sent `value` without a `key` because keys are just to group events with the same key in the same partition to ensure order (useful for keeping states). However, we don't really care about order of the events.\n",
    "\n",
    "The next step is writing a kafka streams job to split the data into to other topics, one that includes the beginning of an event and the other contains the end of an event.\n",
    "\n",
    "This concludes the streaming/ingestion layer. We will finalize the ingestion with the kafka streams in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
