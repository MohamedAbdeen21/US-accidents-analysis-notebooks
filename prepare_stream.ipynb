{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97147a1c",
   "metadata": {},
   "source": [
    "For this project, I decided to use Kafka just to practice connecting Spark with Kafka. Obviously, this project doesn't need Kafka since the data is in a csv. However, we can stream each row based on the start and end time.\n",
    "\n",
    "So, we will make a config.py file that will contain a variable that decides how long should the stream take. In other words, how long should it take to stream the data beginning from the first start date to the last end date to kafka. This variable will be in seconds. \n",
    "\n",
    "Let's try to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/mohamed/.local/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/mohamed/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from config import RUNTIME\n",
    "RUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbcc1e",
   "metadata": {},
   "source": [
    "Now this is out of the way, let's scale the time.\n",
    "\n",
    "From the previous notebook, we know the first start date is '2016-01-14 20:18:33' and the last end date is '2022-01-01 00:00:00'. Let's create a UDF to convert these dates to seconds, scale them to fit inside the RUNTIME window, and stream the entire record (without the end time) with the start time and then stream the end time when its time comes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8fd6d",
   "metadata": {},
   "source": [
    "First, the UDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dda9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d0bec",
   "metadata": {},
   "source": [
    "Oh wait, first the spark instance ... my bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8610c7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/29 20:26:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# We will talk about the two .configs later\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Transform to Stream With Pyspark\") \\\n",
    "    .config('spark.driver.memory','12g') \\\n",
    "    .config('spark.sql.autoBroadcastJoinThreshold','-1') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adef8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def toUnix(date):\n",
    "    return datetime.timestamp(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3353917",
   "metadata": {},
   "source": [
    "Let's test that and verify using an online tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05741ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659119196.067932"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toUnix(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16cf1a",
   "metadata": {},
   "source": [
    "That looks correct. However, the datetime.now() method returns a datetime type, not a string.\n",
    "This means that our input needs to be converted to datetime. Let me redefine the function and I'll be right back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6921495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toUnix(date):\n",
    "    # This is to account for milliseconds, we don't care about such precision\n",
    "    date = date.split('.')[0]\n",
    "    date = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return datetime.timestamp(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98111ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1653087600.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toUnix('2022-05-21 01:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4af686",
   "metadata": {},
   "source": [
    "Perfect!! now, to a UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68696e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "udfToUnix = udf(toUnix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50f2498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string, _c27: string, _c28: string, _c29: string, _c30: string, _c31: string, _c32: string, _c33: string, _c34: string, _c35: string, _c36: string, _c37: string, _c38: string, _c39: string, _c40: string, _c41: string, _c42: string, _c43: string, _c44: string, _c45: string, _c46: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = spark.read.csv('./US_Accidents_Dec21_updated.csv')\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caf37c",
   "metadata": {},
   "source": [
    "The headers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11fe50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Severity: string, Start_Time: string, End_Time: string, Start_Lat: string, Start_Lng: string, End_Lat: string, End_Lng: string, Distance(mi): string, Description: string, Number: string, Street: string, Side: string, City: string, County: string, State: string, Zipcode: string, Country: string, Timezone: string, Airport_Code: string, Weather_Timestamp: string, Temperature(F): string, Wind_Chill(F): string, Humidity(%): string, Pressure(in): string, Visibility(mi): string, Wind_Direction: string, Wind_Speed(mph): string, Precipitation(in): string, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = spark.read.csv('./US_Accidents_Dec21_updated.csv', header = True)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0950b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Severity: string, Start_Time: string, End_Time: string, Start_Lat: string, Start_Lng: string, End_Lat: string, End_Lng: string, Distance(mi): string, Description: string, Number: string, Street: string, Side: string, City: string, County: string, State: string, Zipcode: string, Country: string, Timezone: string, Airport_Code: string, Weather_Timestamp: string, Temperature(F): string, Wind_Chill(F): string, Humidity(%): string, Pressure(in): string, Visibility(mi): string, Wind_Direction: string, Wind_Speed(mph): string, Precipitation(in): string, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string, Start_Time_Unix: float, End_Time_Unix: float]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "main_df = main_df.withColumn('Start_Time_Unix', udfToUnix(main_df['Start_Time']) \\\n",
    "                                             .cast(FloatType()))\n",
    "main_df = main_df.withColumn('End_Time_Unix', udfToUnix(main_df['End_Time'])\n",
    "                                            .cast(FloatType()))\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af760ac",
   "metadata": {},
   "source": [
    "Now we're close! Lets create a table with each unix time and a column that stats the id, of course if two time stamps have the same id then the earlier is the start and the later is the end.\n",
    "\n",
    "But first, we need to clean the ID column! This should be easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c2821c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| ID|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import IntegerType\n",
    "main_df = main_df.withColumn('ID', split(main_df['ID'],'-').getItem(1).cast(IntegerType()))\n",
    "main_df.select('ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cc05e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| ID|Start_Time_Unix|\n",
      "+---+---------------+\n",
      "|  1|   1.45488461E9|\n",
      "|  1|   1.45490624E9|\n",
      "|  2|   1.45492544E9|\n",
      "|  2|   1.45490381E9|\n",
      "|  3|   1.45492659E9|\n",
      "|  3|   1.45490496E9|\n",
      "|  4|   1.45490714E9|\n",
      "|  4|   1.45492877E9|\n",
      "|  5|   1.45491085E9|\n",
      "|  5|   1.45493248E9|\n",
      "|  6|   1.45493376E9|\n",
      "|  6|   1.45491226E9|\n",
      "|  7|   1.45491213E9|\n",
      "|  7|   1.45493376E9|\n",
      "|  8|   1.45492506E9|\n",
      "|  8|   1.45494669E9|\n",
      "|  9|   1.45495565E9|\n",
      "|  9|   1.45493402E9|\n",
      "| 10|   1.45495898E9|\n",
      "| 10|   1.45493734E9|\n",
      "+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Start_Time_Unix: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_df = main_df.select('ID','Start_Time_Unix') \\\n",
    "        .union(main_df.select('ID','End_Time_Unix')) \\\n",
    "        .orderBy('Start_Time_Unix') \\\n",
    "        .orderBy('ID')\n",
    "temp_df.show()\n",
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb2e08",
   "metadata": {},
   "source": [
    "Absolute perfection! Maybe change column name, but other than that ... absolute perfection!\n",
    "\n",
    "You may notice that the second column is basically the same value repeated, but this is multiplied by 10^9.\n",
    "\n",
    "So, all this work and we still didn't get the scaling done. The scaling after this point is easy, we just subtract the earliest time and divide by the latest time and multiply by the RUNTIME.\n",
    "\n",
    "So ... another UDF? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e454d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(min(Time_Unix)=1452795520.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = temp_df.withColumn('Time_Unix',temp_df['Start_Time_Unix'])\n",
    "earliest = temp_df.agg({'Time_Unix':\"min\"}).collect()\n",
    "earliest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71b766",
   "metadata": {},
   "source": [
    "The value is inside a Pyspark row. Don't worry, we can get it out.\n",
    "A simple google search leads us to a few methods to do so, below is two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "181687ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1452795520.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# earliest = earliest[0].__getitem__('min(Start_Time_Unix)')\n",
    "earliest = earliest[0][0]\n",
    "earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e311640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1640988032.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = temp_df.agg({\"Time_Unix\":\"max\"}).collect()\n",
    "latest = latest[0][0]\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f505192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(unix):\n",
    "    return ((unix - earliest) / (latest - earliest))*RUNTIME \n",
    "\n",
    "# We can specify the return type of the udf, instead of the approach we used before\n",
    "udfScaling = udf(scale,FloatType())\n",
    "temp_df = temp_df.withColumn('Stream_Time',udfScaling(temp_df['Time_Unix']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a923a9",
   "metadata": {},
   "source": [
    "A job well done!\n",
    "Just one final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3e2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.withColumnRenamed('ID','temp_id')\n",
    "to_delete = ('Start_Time_Unix','End_Time_Unix','Time_Unix',\"temp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "663db3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "stream_df = temp_df.join(main_df, temp_df.temp_id == main_df.ID) \\\n",
    "            .drop(*to_delete)\n",
    "stream_df = stream_df.orderBy('Stream_Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b92e8f",
   "metadata": {},
   "source": [
    "That was alot of work. Now we will just write to a parquet file and read from it in the streaming script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "706167e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/29 20:27:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "targetfolder = './parquet/'\n",
    "stream_df.coalesce(1).write.parquet(targetfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff30e2e",
   "metadata": {},
   "source": [
    "What is coalesce? Basically I needed to write out to a parquet file, removing the coalesce(1) from the last line will give us a folder with multiple parquet files inside; that is a file for each partition.\n",
    "\n",
    "To ask pyspark for a single file we need to reduce the number of partitions using repartition or coalesce. They both can reduce number of partitions; however, coalesce is more optimized since it can only reduce number of partitions (unlike repartition). \n",
    "\n",
    "We had to set the two configs in SparkSession call, because we ran out of memory when trying to write out the file. You can google their names and see what each one does.\n",
    "\n",
    "Now to rename the output file so that we can read it from the streaming script without this weird name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8176a344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part-00000-bba88a21-deef-4b42-9b8d-c0835cc667da-c000.snappy.parquet',\n",
       " '.part-00000-bba88a21-deef-4b42-9b8d-c0835cc667da-c000.snappy.parquet.crc',\n",
       " '_SUCCESS',\n",
       " '._SUCCESS.crc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(targetfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443c495",
   "metadata": {},
   "source": [
    "What even is that name, spark? It's ok, we got it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50b5b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir(targetfolder)[0]\n",
    "os.rename(f'{targetfolder}{filename}', f'{targetfolder}stream_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e1509",
   "metadata": {},
   "source": [
    "And we're done for this one. Let's now stream that dataframe to a kafka topic. Woohoo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
