{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97147a1c",
   "metadata": {},
   "source": [
    "For this project, I decided to use Kafka just to practice connecting Spark with Kafka. Obviously, this project doesn't need Kafka since the data is in a csv. However, we can stream each row based on the start and end time.\n",
    "\n",
    "So, we will make a config.py file that will contain a variable that decides how long should the stream take. In other words, how long should it take to stream the data beginning from the first start date to the last end date to kafka. This variable will be in seconds. \n",
    "\n",
    "Let's try to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: pyspark in /home/mohamed/.local/lib/python3.10/site-packages (3.3.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/mohamed/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from config import RUNTIME\n",
    "RUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbcc1e",
   "metadata": {},
   "source": [
    "Now this is out of the way, let's scale the time.\n",
    "\n",
    "From the previous notebook, we know the first start date is '2016-01-14 20:18:33' and the last end date is '2022-01-01 00:00:00'. Let's convert these dates to seconds, scale them to fit inside the RUNTIME window, and stream the entire record (without the end time) with the start time and then stream the end time when its time comes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8610c7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/01 16:27:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Using 6 cores instead of * due to the limited memory on my machine, less threads, less memory\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[6]') \\\n",
    "    .appName(\"Transform to Stream With Pyspark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adef8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3353917",
   "metadata": {},
   "source": [
    "Let's test that and verify using an online tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f50f2498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string, _c27: string, _c28: string, _c29: string, _c30: string, _c31: string, _c32: string, _c33: string, _c34: string, _c35: string, _c36: string, _c37: string, _c38: string, _c39: string, _c40: string, _c41: string, _c42: string, _c43: string, _c44: string, _c45: string, _c46: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = spark.read.csv('./US_Accidents_Dec21_updated.csv')\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caf37c",
   "metadata": {},
   "source": [
    "The headers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11fe50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Severity: string, Start_Time: string, End_Time: string, Start_Lat: string, Start_Lng: string, End_Lat: string, End_Lng: string, Distance(mi): string, Description: string, Number: string, Street: string, Side: string, City: string, County: string, State: string, Zipcode: string, Country: string, Timezone: string, Airport_Code: string, Weather_Timestamp: string, Temperature(F): string, Wind_Chill(F): string, Humidity(%): string, Pressure(in): string, Visibility(mi): string, Wind_Direction: string, Wind_Speed(mph): string, Precipitation(in): string, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = spark.read.csv('./US_Accidents_Dec21_updated.csv', header = True)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0950b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Severity: string, Start_Time: string, End_Time: string, Start_Lat: string, Start_Lng: string, End_Lat: string, End_Lng: string, Distance(mi): string, Description: string, Number: string, Street: string, Side: string, City: string, County: string, State: string, Zipcode: string, Country: string, Timezone: string, Airport_Code: string, Weather_Timestamp: string, Temperature(F): string, Wind_Chill(F): string, Humidity(%): string, Pressure(in): string, Visibility(mi): string, Wind_Direction: string, Wind_Speed(mph): string, Precipitation(in): string, Weather_Condition: string, Amenity: string, Bump: string, Crossing: string, Give_Way: string, Junction: string, No_Exit: string, Railway: string, Roundabout: string, Station: string, Stop: string, Traffic_Calming: string, Traffic_Signal: string, Turning_Loop: string, Sunrise_Sunset: string, Civil_Twilight: string, Nautical_Twilight: string, Astronomical_Twilight: string, Start_Time_Unix: bigint, End_Time_Unix: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType, LongType, datetime\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "main_df = main_df.withColumn('Start_Time_Unix', unix_timestamp(split(main_df['Start_Time'],'\\.').getItem(0)))\n",
    "main_df = main_df.withColumn('End_Time_Unix', unix_timestamp(split(main_df['End_Time'],'\\.').getItem(0)))\n",
    "\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af760ac",
   "metadata": {},
   "source": [
    "We used the split because some of the values in the `Start_Time` and `End_Time` columns had a millisecond. We cut these off with split and remember to escape the `.` with `\\` .\n",
    "\n",
    "Now we're close! Lets create a table with each unix time and a column that stats the id, of course if two time stamps have the same id then the earlier is the start and the later is the end.\n",
    "\n",
    "But first, we need to clean the ID column! This should be easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2821c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------+\n",
      "| ID|Start_Time_Unix|End_Time_Unix|\n",
      "+---+---------------+-------------+\n",
      "|  1|     1454884628|   1454906228|\n",
      "|  2|     1454903780|   1454925380|\n",
      "|  3|     1454904939|   1454926539|\n",
      "|  4|     1454907105|   1454928705|\n",
      "|  5|     1454910823|   1454932423|\n",
      "|  6|     1454912217|   1454933817|\n",
      "|  7|     1454912141|   1454933741|\n",
      "|  8|     1454925106|   1454946706|\n",
      "|  9|     1454933997|   1454955597|\n",
      "| 10|     1454937403|   1454959003|\n",
      "| 11|     1454939030|   1454960630|\n",
      "| 12|     1454943057|   1454964657|\n",
      "| 13|     1454945259|   1454966859|\n",
      "| 14|     1454945418|   1454967018|\n",
      "| 15|     1454947871|   1454969471|\n",
      "| 16|     1454947871|   1454969471|\n",
      "| 17|     1454953662|   1454975262|\n",
      "| 18|     1454953662|   1454975262|\n",
      "| 19|     1454955202|   1454976802|\n",
      "| 20|     1454958017|   1454979617|\n",
      "+---+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import IntegerType\n",
    "main_df = main_df.withColumn('ID', split(main_df['ID'],'-').getItem(1).cast(IntegerType()))\n",
    "main_df.select('ID','Start_Time_Unix','End_Time_Unix').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc05e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>            (14 + 4) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+\n",
      "|Start| ID|Start_Time_Unix|\n",
      "+-----+---+---------------+\n",
      "|Start|  1|     1454884628|\n",
      "|  End|  1|     1454906228|\n",
      "|  End|  2|     1454925380|\n",
      "|Start|  2|     1454903780|\n",
      "|  End|  3|     1454926539|\n",
      "|Start|  3|     1454904939|\n",
      "|Start|  4|     1454907105|\n",
      "|  End|  4|     1454928705|\n",
      "|  End|  5|     1454932423|\n",
      "|Start|  5|     1454910823|\n",
      "|  End|  6|     1454933817|\n",
      "|Start|  6|     1454912217|\n",
      "|Start|  7|     1454912141|\n",
      "|  End|  7|     1454933741|\n",
      "|Start|  8|     1454925106|\n",
      "|  End|  8|     1454946706|\n",
      "|Start|  9|     1454933997|\n",
      "|  End|  9|     1454955597|\n",
      "|Start| 10|     1454937403|\n",
      "|  End| 10|     1454959003|\n",
      "+-----+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Start: string (nullable = false)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Start_Time_Unix: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "temp_df = main_df.select(lit('Start'),'ID','Start_Time_Unix') \\\n",
    "        .union(main_df.select(lit('End'),'ID','End_Time_Unix')) \\\n",
    "        .orderBy('Start_Time_Unix') \\\n",
    "        .orderBy('ID')\n",
    "temp_df.show()\n",
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb2e08",
   "metadata": {},
   "source": [
    "Absolute perfection! Maybe change column name, but other than that ... absolute perfection!\n",
    "\n",
    "You may notice that the second column is basically the same value repeated, but this is multiplied by 10^9.\n",
    "\n",
    "So, all this work and we still didn't get the scaling done. The scaling after this point is easy, we just subtract the earliest time and divide by the latest time and multiply by the RUNTIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e454d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(min(Time_Unix)=1452795513)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = temp_df.withColumn('Time_Unix',temp_df['Start_Time_Unix'])\n",
    "earliest = temp_df.agg({'Time_Unix':\"min\"}).collect()\n",
    "earliest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71b766",
   "metadata": {},
   "source": [
    "The value is inside a Pyspark row. Don't worry, we can get it out.\n",
    "A simple google search leads us to a few methods to do so, below is two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181687ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1452795513"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# earliest = earliest[0].__getitem__('min(Time_Unix)')\n",
    "earliest = earliest[0][0]\n",
    "earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e311640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1640988000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = temp_df.agg({\"Time_Unix\":\"max\"}).collect()\n",
    "latest = latest[0][0]\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f505192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify the return type of the udf, instead of the approach we used before\n",
    "temp_df = temp_df.withColumn('Stream_Time',\n",
    "            (temp_df['Time_Unix'].cast(FloatType()) - earliest) * RUNTIME / (latest-earliest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a923a9",
   "metadata": {},
   "source": [
    "A job well done!\n",
    "Just one final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c3e2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.withColumnRenamed('ID','temp_id')\n",
    "to_delete = ('Start_Time_Unix','End_Time_Unix','Time_Unix',\"temp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663db3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = temp_df.join(main_df, temp_df.temp_id == main_df.ID) \\\n",
    "            .drop(*to_delete)\n",
    "stream_df = stream_df.orderBy('Stream_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bebd20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5690684"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b92e8f",
   "metadata": {},
   "source": [
    "That was alot of work. Now we will just write to a parquet file and read from it in the streaming script. First, we will repartition the dataframe. Why? Because we don't want to run out of memory and more partitions means each partition is smaller (and more balanced) and less memory pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9393a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/01 16:28:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:================================================>         (5 + 1) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|SPARK_PARTITION_ID()|  count|\n",
      "+--------------------+-------+\n",
      "|                   0| 840255|\n",
      "|                   1| 919366|\n",
      "|                   2| 929988|\n",
      "|                   3| 898228|\n",
      "|                   4| 951716|\n",
      "|                   5|1151131|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   0|113814|\n",
      "|                   1|113814|\n",
      "|                   2|113814|\n",
      "|                   3|113814|\n",
      "|                   4|113814|\n",
      "|                   5|113814|\n",
      "|                   6|113814|\n",
      "|                   7|113814|\n",
      "|                   8|113814|\n",
      "|                   9|113815|\n",
      "|                  10|113814|\n",
      "|                  11|113814|\n",
      "|                  12|113814|\n",
      "|                  13|113815|\n",
      "|                  14|113815|\n",
      "|                  15|113815|\n",
      "|                  16|113815|\n",
      "|                  17|113815|\n",
      "|                  18|113815|\n",
      "|                  19|113814|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "stream_df.groupBy(spark_partition_id()).count().show()\n",
    "stream_df = stream_df.repartition(50)\n",
    "stream_df.groupBy(spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "706167e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "targetfolder = './parquet/'\n",
    "stream_df.coalesce(1).write.parquet(targetfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff30e2e",
   "metadata": {},
   "source": [
    "What is coalesce? Basically I needed to write out to a parquet file, removing the coalesce(1) from the last line will give us a folder with multiple parquet files inside; that is a file for each partition.\n",
    "\n",
    "To ask pyspark for a single file we need to reduce the number of partitions using repartition or coalesce. They both can reduce number of partitions; however, coalesce is more optimized since it can only reduce number of partitions (unlike repartition). \n",
    "\n",
    "Now to rename the output file so that we can read it from the streaming script without this weird name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37c79f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part-00000-35dc0053-edcc-41a9-be97-ebf07931ccdf-c000.snappy.parquet',\n",
       " '.part-00000-35dc0053-edcc-41a9-be97-ebf07931ccdf-c000.snappy.parquet.crc',\n",
       " '_SUCCESS',\n",
       " '._SUCCESS.crc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(targetfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443c495",
   "metadata": {},
   "source": [
    "What even is that name, spark? It's ok, we got it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e80a0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir(targetfolder)[0]\n",
    "os.rename(f'{targetfolder}{filename}', f'{targetfolder}stream_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e1509",
   "metadata": {},
   "source": [
    "And we're done for this one. Let's now stream that dataframe to a kafka topic. Woohoo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
